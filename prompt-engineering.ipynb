{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Ways of Using LLM APIs\n",
    "\n",
    "### 01. OpenAI APIs\n",
    "    - You can use OpenAI APIs directly by using OpenAI API key\n",
    "    - OpenAI keys will available in your OpenAI account\n",
    "\n",
    "### 02. Together AI APIs\n",
    "    - Instead of using OpenAI API key, you can use Together AI API key\n",
    "    - Together AI has hosted more than 100+ Models in both closed and open source\n",
    "\n",
    "### 03. Azure APIs\n",
    "    - You can access OpenAI APIs through Azurem instead of using OpenAI API directly\n",
    "    - This is the best way to use OpenAI APIs in production\n",
    "    - It will secure your data you input via prompting\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, together\n",
    "from openai import OpenAI, ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credential.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['TOGETHER_AI_API'] = credentials['TOGETHER_AI_API']\n",
    "os.environ['OPENAI_API_KEY'] = credentials['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Basic Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01. Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=25\n",
    "    )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. Using Together AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austism/chronos-hermes-13b\n",
      "DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
      "EleutherAI/llemma_7b\n",
      "Gryphe/MythoMax-L2-13b\n",
      "Meta-Llama/Llama-Guard-7b\n",
      "Nexusflow/NexusRaven-V2-13B\n",
      "NousResearch/Nous-Capybara-7B-V1p9\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "NousResearch/Nous-Hermes-Llama2-70b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NumbersStation/nsql-llama-2-7B\n",
      "Open-Orca/Mistral-7B-OpenOrca\n",
      "Phind/Phind-CodeLlama-34B-Python-v1\n",
      "Phind/Phind-CodeLlama-34B-v2\n",
      "SG161222/Realistic_Vision_V3.0_VAE\n",
      "Undi95/ReMM-SLERP-L2-13B\n",
      "Undi95/Toppy-M-7B\n",
      "WizardLM/WizardCoder-15B-V1.0\n",
      "WizardLM/WizardCoder-Python-34B-V1.0\n",
      "WizardLM/WizardLM-13B-V1.2\n",
      "WizardLM/WizardLM-70B-V1.0\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "huggyllama/llama-65b\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-7b-v1.5\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "openchat/openchat-3.5-1210\n",
      "prompthero/openjourney\n",
      "runwayml/stable-diffusion-v1-5\n",
      "stabilityai/stable-diffusion-2-1\n",
      "stabilityai/stable-diffusion-xl-base-1.0\n",
      "teknium/OpenHermes-2-Mistral-7B\n",
      "teknium/OpenHermes-2p5-Mistral-7B\n",
      "togethercomputer/CodeLlama-13b-Instruct\n",
      "togethercomputer/CodeLlama-13b-Python\n",
      "togethercomputer/CodeLlama-13b\n",
      "togethercomputer/CodeLlama-34b-Instruct\n",
      "togethercomputer/CodeLlama-34b-Python\n",
      "togethercomputer/CodeLlama-34b\n",
      "togethercomputer/CodeLlama-7b-Instruct\n",
      "togethercomputer/CodeLlama-7b-Python\n",
      "togethercomputer/CodeLlama-7b\n",
      "togethercomputer/GPT-JT-6B-v1\n",
      "togethercomputer/GPT-JT-Moderation-6B\n",
      "togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/Pythia-Chat-Base-7B-v0.16\n",
      "togethercomputer/Qwen-7B-Chat\n",
      "togethercomputer/Qwen-7B\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/StripedHyena-Hessian-7B\n",
      "togethercomputer/StripedHyena-Nous-7B\n",
      "togethercomputer/alpaca-7b\n",
      "togethercomputer/falcon-40b-instruct\n",
      "togethercomputer/falcon-40b\n",
      "togethercomputer/falcon-7b-instruct\n",
      "togethercomputer/falcon-7b\n",
      "togethercomputer/llama-2-13b-chat\n",
      "togethercomputer/llama-2-13b\n",
      "togethercomputer/llama-2-70b-chat\n",
      "togethercomputer/llama-2-70b\n",
      "togethercomputer/llama-2-7b-chat\n",
      "togethercomputer/llama-2-7b\n",
      "upstage/SOLAR-0-70b-16bit\n",
      "wavymulder/Analog-Diffusion\n",
      "zero-one-ai/Yi-34B-Chat\n",
      "zero-one-ai/Yi-34B\n",
      "zero-one-ai/Yi-6B\n",
      "HuggingFaceH4/zephyr-7b-beta\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "EleutherAI/pythia-1b-v0\n",
      "togethercomputer/codegen2-16B\n",
      "togethercomputer/replit-code-v1-3b\n",
      "togethercomputer/mpt-7b\n",
      "togethercomputer/mpt-30b-chat\n",
      "google/flan-t5-xxl\n",
      "google/flan-t5-xl\n",
      "togethercomputer/mpt-7b-instruct\n",
      "NumbersStation/nsql-6B\n",
      "togethercomputer/Koala-7B\n",
      "EleutherAI/pythia-6.9b\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "EleutherAI/gpt-neox-20b\n",
      "EleutherAI/pythia-2.8b-v0\n",
      "NousResearch/Nous-Hermes-13b\n",
      "togethercomputer/guanaco-65b\n",
      "OpenAssistant/oasst-sft-6-llama-30b-xor\n",
      "Salesforce/instructcodet5p-16b\n",
      "lmsys/fastchat-t5-3b-v1.0\n",
      "huggyllama/llama-7b\n",
      "OpenAssistant/stablelm-7b-sft-v7-epoch-3\n",
      "EleutherAI/pythia-12b-v0\n",
      "togethercomputer/mpt-7b-chat\n",
      "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "EleutherAI/gpt-j-6b\n",
      "lmsys/vicuna-7b-v1.3\n",
      "togethercomputer/codegen2-7B\n",
      "togethercomputer/guanaco-13b\n",
      "lmsys/vicuna-13b-v1.3\n",
      "huggyllama/llama-13b\n",
      "HuggingFaceH4/starchat-alpha\n",
      "huggyllama/llama-30b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "defog/sqlcoder\n",
      "bigcode/starcoder\n",
      "databricks/dolly-v2-7b\n",
      "togethercomputer/guanaco-33b\n",
      "togethercomputer/Koala-13B\n",
      "togethercomputer/guanaco-7b\n"
     ]
    }
   ],
   "source": [
    "together.api_key = os.environ[\"TOGETHER_AI_API\"]\n",
    "\n",
    "# list available models and descriptons\n",
    "models = together.Models.list()\n",
    "for m in models:\n",
    "    print(m['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api.together.xyz/instances/start?model=togethercomputer/llama-2-13b-chat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtogether\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtogethercomputer/llama-2-13b-chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LLM_project/llm_env/lib/python3.9/site-packages/together/models.py:56\u001b[0m, in \u001b[0;36mModels.start\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     53\u001b[0m     model_url \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murljoin(\n\u001b[1;32m     54\u001b[0m         together\u001b[38;5;241m.\u001b[39mapi_base_instances, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart?model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     )\n\u001b[0;32m---> 56\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_post_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_to_dict(response)\n",
      "File \u001b[0;32m~/Desktop/LLM_project/llm_env/lib/python3.9/site-packages/together/utils.py:114\u001b[0m, in \u001b[0;36mcreate_post_request\u001b[0;34m(url, headers, json, stream, check_auth, api_key)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m together\u001b[38;5;241m.\u001b[39mResponseError(e)\n\u001b[0;32m--> 114\u001b[0m \u001b[43mresponse_status_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/LLM_project/llm_env/lib/python3.9/site-packages/together/utils.py:87\u001b[0m, in \u001b[0;36mresponse_status_exception\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid authentication credentials\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LLM_project/llm_env/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://api.together.xyz/instances/start?model=togethercomputer/llama-2-13b-chat"
     ]
    }
   ],
   "source": [
    "together.Models.start(\"togethercomputer/llama-2-13b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_model_together(USER_MESSAGE):\n",
    "    output = together.Complete.create(\n",
    "                                    USER_MESSAGE,\n",
    "                                    model=\"togethercomputer/llama-2-13b-chat\",\n",
    "                                    max_tokens=20,\n",
    "                                    temperature=0,\n",
    "                                    )\n",
    "    text = output['output']['choices'][0]['text'].replace('\\n', '   ').strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current president of the United States is Joe Biden. He was inaugurated on January'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_model_together('Who is the president of the United States?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    "    )\n",
    "    return str(response.choices[0].message.content)\n",
    "\n",
    "# def completion_model(USER_MESSAGE):\n",
    "#     response = client.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful sentiment analyzer. Classify the text into neutral, negative or positive.\"},\n",
    "#         {\"role\": \"user\", \"content\": USER_MESSAGE}\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "#     max_tokens=10\n",
    "#     )\n",
    "#     return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_model('''\n",
    "Classify the text into neutral, negative or positive. \n",
    "\n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10\n",
    "    )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_model('''\n",
    "                 \n",
    "Classify the TEXT into neutral, negative or positive. use below examples for few shots. only return neutral, negative or positive\n",
    "                 \n",
    "This is awesome! // Positive\n",
    "This is bad! // Negative\n",
    "That movie was ok. // Neutral\n",
    "                 \n",
    "\n",
    "The day was not nice.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) COT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=1000\n",
    "    )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29 fruits'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_model('''               \n",
    "I went to the market and bought 10 apples. \n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1.             \n",
    "How many apples did I remain with if I neglect the one I ate? Do not include the method of solving the problem.\n",
    "                \n",
    "Answer : 11 Apples \n",
    "                 \n",
    "I went to the shop with 10 bags. each bag can store 3 fruits \n",
    "I bought fruits until all the bags were full. I gave 5 fruits to my friend and ate 1.\n",
    "How many fruits did I remain with if I neglect the one I ate? Do not include the method of solving the problem. \n",
    "\n",
    "Answer :\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You started with 10 bags, and each bag can store 3 fruits. So you had a total of 10 x 3 = 30 fruits.\n",
      "After giving 5 fruits to your friend, you had 30 - 5 = 25 fruits.\n",
      "You then ate 1 fruit, but since we are neglecting it, you remained with 25 fruits.\n"
     ]
    }
   ],
   "source": [
    "print(completion_model('''               \n",
    "I went to the market and bought 10 apples. \n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1.             \n",
    "How many apples did I remain with if I neglect the one I ate? Do not include the method of solving the problem.\n",
    "                \n",
    "Answer : I started with 10 apples. \n",
    "         After giving 2 to the neighbor and 2 to the repairman. 10 - 2 - 2 = 6.\n",
    "         I bought 5 more apples. 6 + 5 = 11. I ate 1 but since we are neglecting it, I remained with 11 apples.\n",
    "                 \n",
    "I went to the shop with 10 bags. each bag can store 3 fruits \n",
    "I bought fruits until all the bags were full. I gave 5 fruits to my friend and ate 1.\n",
    "How many fruits did I remain with if I neglect the one I ate? Do not include the method of solving the problem. \n",
    "\n",
    "Answer :\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE}\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    max_tokens=100\n",
    "    )\n",
    "    return str(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You started with 10 bags, each bag can store 3 fruits. So you had 10 x 3 = 30 fruits at the beginning.\n",
      "You filled all the bags with fruits, so you used all 30 fruits.\n",
      "Then you gave 5 fruits to your friend and ate 1, so you used 5 + 1 = 6 fruits.\n",
      "Since we are neglecting the one you ate, you remained with 30 - 6 = 24 fruits.\n"
     ]
    }
   ],
   "source": [
    "print(completion_model('''               \n",
    "I went to the market and bought 10 apples. \n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1.             \n",
    "How many apples did I remain with if I neglect the one I ate? Do not include the method of solving the problem.\n",
    "                \n",
    "Answer : I started with 10 apples. \n",
    "         After giving 2 to the neighbor and 2 to the repairman. 10 - 2 - 2 = 6.\n",
    "         I bought 5 more apples. 6 + 5 = 11. I ate 1 but since we are neglecting it, I remained with 11 apples.\n",
    "                 \n",
    "I went to the shop with 10 bags. each bag can store 3 fruits \n",
    "I bought fruits until all the bags were full. I gave 5 fruits to my friend and ate 1.\n",
    "How many fruits did I remain with if I neglect the one I ate? Do not include the method of solving the problem. \n",
    "\n",
    "Answer :\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You had 10 bags and each bag can store 3 fruits, so you had a total capacity of 10 bags * 3 fruits/bag = 30 fruits.\n",
      "You filled all the bags with fruits, so you had 30 fruits.\n",
      "You gave 5 fruits to your friend, so you had 30 fruits - 5 fruits = 25 fruits left.\n",
      "You ate 1 fruit, but since we are neglecting it, you remained with 25 fruits.\n"
     ]
    }
   ],
   "source": [
    "print(completion_model('''               \n",
    "I went to the market and bought 10 apples. \n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1.             \n",
    "How many apples did I remain with if I neglect the one I ate? Do not include the method of solving the problem.\n",
    "                \n",
    "Answer : I started with 10 apples. \n",
    "         After giving 2 to the neighbor and 2 to the repairman. 10 - 2 - 2 = 6.\n",
    "         I bought 5 more apples. 6 + 5 = 11. I ate 1 but since we are neglecting it, I remained with 11 apples.\n",
    "                 \n",
    "I went to the shop with 10 bags. each bag can store 3 fruits \n",
    "I bought fruits until all the bags were full. I gave 5 fruits to my friend and ate 1.\n",
    "How many fruits did I remain with if I neglect the one I ate? Do not include the method of solving the problem. \n",
    "\n",
    "Answer :\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) ToT Prompting\n",
    "\n",
    "Imagine three different experts are answering this question. </br>\n",
    "All experts will write down 1 step of their thinking, </br>\n",
    "then share it with the group. </br>\n",
    "Then all experts will go on to the next step, etc. </br>\n",
    "If any expert realises they're wrong at any point then they leave. </br>\n",
    "The question is..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
